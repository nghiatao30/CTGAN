{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd94723-99a0-4530-b8f5-1e9be6299072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctgan import CTGAN\n",
    "from ctgan import load_demo\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "print(torch.cuda.is_available())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d359f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load log file\n",
    "def load_log(log_file):\n",
    "    with open(log_file, 'r') as f:\n",
    "        log_data = [json.loads(line.strip()) for line in f]\n",
    "    return pd.DataFrame(log_data)\n",
    "\n",
    "# Add annotations for bars and lines\n",
    "def add_annotations(ax, x, y, is_line=False):\n",
    "    for i, value in enumerate(y):\n",
    "        if is_line:\n",
    "            ax.annotate(f\"{value:.2f}\", xy=(x[i], y[i]), ha='center', va='bottom', fontsize=9, color='red')\n",
    "        else:\n",
    "            ax.annotate(f\"{value:.2f}\", xy=(x[i], y[i]), ha='center', va='bottom', fontsize=9, color='blue')\n",
    "\n",
    "# Plot combined metrics\n",
    "def plot_metrics_comparison(df, save_path):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    x_labels = [f\"{row['model']} ({row['test_dataset']})\" for _, row in df.iterrows()]\n",
    "    x_positions = range(len(x_labels))\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Bar: Training Time\n",
    "    bar_width = 0.4\n",
    "    bars = ax1.bar(x_positions, df['train_time'], width=bar_width, alpha=0.6, label=\"Training Time (s)\", color='blue')\n",
    "    ax1.set_ylabel(\"Training Time (s)\", color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.set_xticks(x_positions)\n",
    "    ax1.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "\n",
    "    # Add annotations to bars\n",
    "    add_annotations(ax1, x_positions, df['train_time'])\n",
    "\n",
    "    # Line: Accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x_positions, df['Accuracy'], marker='o', color='red', label=\"Accuracy\")\n",
    "    ax2.set_ylabel(\"Accuracy\", color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    # Add annotations to line\n",
    "    add_annotations(ax2, x_positions, df['Accuracy'], is_line=True)\n",
    "\n",
    "    # Title and Legend\n",
    "    plt.title(\"Training Time and Accuracy Comparison Across Models\")\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Combined metrics comparison chart saved at {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# Additional detailed analysis\n",
    "def analyze_metrics(df):\n",
    "    print(\"\\nDetailed Metric Analysis:\")\n",
    "    for metric in ['Accuracy', 'F1 Score', 'AUC']:\n",
    "        best_model = df.loc[df[metric].idxmax()]\n",
    "        print(f\"Best model for {metric}: {best_model['model']} ({best_model['train_dataset']} -> {best_model['test_dataset']}) \"\n",
    "              f\"with {metric}: {best_model[metric]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e684d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = 'experiment_log_with_time.json'  # Path to the log file\n",
    "save_path = 'combined_metrics_comparison.png'  # Path to save the chart\n",
    "df = load_log(log_file)\n",
    "\n",
    "for col in ['train_time', 'Accuracy', 'F1 Score', 'AUC']:\n",
    "    df[col] = df[col].astype(float)\n",
    "    plot_metrics_comparison(df, save_path)\n",
    "    analyze_metrics(df)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
