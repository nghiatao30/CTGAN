{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd94723-99a0-4530-b8f5-1e9be6299072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "print(torch.cuda.is_available())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76e3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Load datasets\n",
    "def load_datasets():\n",
    "    # Replace with your actual dataset paths\n",
    "    data1 = pd.read_csv('C:\\Final Project in Uni\\CTGAN\\expe\\infogandataset/2023/final_2023_dataset.csv')\n",
    "    data2 = pd.read_csv('C:\\Final Project in Uni\\CTGAN\\expe/realdata/2023\\Clean_2023data.csv')\n",
    "    return data1, data2\n",
    "\n",
    "# Preprocess datasets\n",
    "def preprocess_data(data, target_column):\n",
    "    # Convert target column from string to numeric\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[target_column] = label_encoder.fit_transform(data[target_column])\n",
    "    \n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y, label_encoder\n",
    "\n",
    "# Stratified sampling by class percentage\n",
    "def stratified_sampling(X, y, sample_percentage):\n",
    "    data = pd.concat([X, y], axis=1)\n",
    "    sampled_data = data.groupby(y.name, group_keys=False).apply(lambda x: x.sample(frac=sample_percentage, random_state=42))\n",
    "    X_sampled = sampled_data.iloc[:, :-1]\n",
    "    y_sampled = sampled_data.iloc[:, -1]\n",
    "    return X_sampled, y_sampled\n",
    "\n",
    "# Build MLP model\n",
    "def build_mlp(input_dim, n_classes):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(n_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='sparse_categorical_crossentropy',  # For integer labels\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, X_test, y_test, n_classes, is_keras=False):\n",
    "    if is_keras:\n",
    "        y_pred_probs = model.predict(X_test)\n",
    "        y_pred = y_pred_probs.argmax(axis=1)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_probs = model.predict_proba(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    if n_classes > 2:\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(range(n_classes))  # Ensure all classes are included\n",
    "        y_test_bin = lb.transform(y_test)  # Binarize y_test\n",
    "        auc = roc_auc_score(y_test_bin, y_pred_probs, multi_class='ovr', average='weighted')\n",
    "    else:\n",
    "        auc = roc_auc_score(y_test, y_pred_probs[:, 1]) if n_classes == 2 else None\n",
    "\n",
    "# Save results to log\n",
    "def log_results(log_file, model_name, train_dataset, test_dataset, metrics, train_time):\n",
    "    log_entry = {\n",
    "        \"model\": model_name,\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"test_dataset\": test_dataset,\n",
    "        \"metrics\": metrics,\n",
    "        \"train_time\": train_time\n",
    "    }\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(json.dumps(log_entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d2cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'Target'  # Replace with your target column\n",
    "log_file = 'C:\\Final Project in Uni\\CTGAN\\expe\\infogandataset/2023experiment_log_infogan2023.json'\n",
    "sample_percentages = [0.1, 0.2, 0.4, 0.8]  # Percentages for stratified sampling\n",
    "    \n",
    "    # Load datasets\n",
    "data1, data2 = load_datasets()\n",
    "X1, y1, label_encoder1 = preprocess_data(data1, target_column)\n",
    "X2, y2, label_encoder2 = preprocess_data(data2, target_column)\n",
    "n_classes = len(label_encoder1.classes_)  # Number of classes\n",
    "    \n",
    "    # Models to train\n",
    "models = {\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "        \"MLP\": None  # Special handling for MLP\n",
    "    }\n",
    "\n",
    "    # Loop through sample percentages\n",
    "for sample_percentage in sample_percentages:\n",
    "    print(f\"Running experiments with {int(sample_percentage * 100)}% of the data per class...\")\n",
    "    \n",
    "    # Stratified sampling\n",
    "    X1_sampled, y1_sampled = stratified_sampling(X1, y1, sample_percentage)\n",
    "    #X2_sampled, y2_sampled = stratified_sampling(X2, y2, sample_percentage)\n",
    "\n",
    "    # Split dataset 2 into train/test for evaluation\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
    "        X2, y2, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale data for MLP\n",
    "    scaler1 = StandardScaler()\n",
    "    scaler2 = StandardScaler()\n",
    "    #X1_scaled = scaler1.fit_transform(X1_sampled) \n",
    "    X1_scaled = X1_sampled\n",
    "    X2_train_scaled = scaler2.fit_transform(X2_train)\n",
    "    X2_test_scaled = scaler2.transform(X2_test)\n",
    "\n",
    "    # **Train on dataset 1 and test on test split of dataset 2**\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on dataset1_{int(sample_percentage * 100)}% and testing on dataset2_{int(sample_percentage * 100)}% (test split)\")\n",
    "        start_time = time.time()  # Start timer\n",
    "        if model_name == \"MLP\":\n",
    "            mlp = build_mlp(X1_scaled.shape[1], n_classes)\n",
    "            mlp.fit(X1_scaled, y1_sampled, epochs=10, batch_size=32, verbose=0)\n",
    "            train_time = time.time() - start_time  # End timer\n",
    "            metrics = evaluate_model(mlp, X2_test_scaled, y2_test, n_classes, is_keras=True)\n",
    "        else:\n",
    "            model.fit(X1_sampled, y1_sampled)\n",
    "            train_time = time.time() - start_time  # End timer\n",
    "            metrics = evaluate_model(model, X2_test_scaled, y2_test, n_classes)\n",
    "        log_results(log_file, model_name, f\"info2023_{int(sample_percentage * 100)}%\", f\"realdata2023_{int(sample_percentage * 100)}% (test split)\", metrics, train_time)\n",
    "\n",
    "    # **Train and test entirely on dataset 2 (train/test split)**\n",
    "    # for model_name, model in models.items():\n",
    "    #     print(f\"Training and testing {model_name} entirely on dataset2_{int(sample_percentage * 100)}% (train/test split)\")\n",
    "    #     start_time = time.time()  # Start timer\n",
    "    #     if model_name == \"MLP\":\n",
    "    #         mlp = build_mlp(X2_train_scaled.shape[1], n_classes)\n",
    "    #         mlp.fit(X2_train_scaled, y2_train, epochs=10, batch_size=32, verbose=0)\n",
    "    #         train_time = time.time() - start_time  # End timer\n",
    "    #         metrics = evaluate_model(mlp, X2_test, y2_test, n_classes, is_keras=True)\n",
    "    #     else:\n",
    "    #         model.fit(X2_train, y2_train)\n",
    "    #         train_time = time.time() - start_time  # End timer\n",
    "    #         metrics = evaluate_model(model, X2_test, y2_test, n_classes)\n",
    "    #     log_results(log_file, model_name, f\"ctabgan20_{int(sample_percentage * 100)}% (train)\", f\"realdata20_{int(sample_percentage * 100)}% (test)\", metrics, train_time)\n",
    "\n",
    "print(f\"Experiment results with multiclass classification logged in {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
