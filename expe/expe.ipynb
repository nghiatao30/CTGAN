{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd94723-99a0-4530-b8f5-1e9be6299072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "print(torch.cuda.is_available())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76e3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load datasets\n",
    "def load_datasets():\n",
    "    # Replace with your actual dataset paths\n",
    "    data1 = pd.read_csv('C:\\Final Project in Uni\\CTGAN\\expe/tabgandataset/2017\\ctabgan2017.csv')\n",
    "    data2 = pd.read_csv('C:\\Final Project in Uni\\CTGAN\\expe/realdata\\/2017\\Clean_2017data.csv')\n",
    "    return data1, data2\n",
    "\n",
    "# Preprocess datasets\n",
    "def preprocess_data(data, target_column):\n",
    "    # Convert target column from string to numeric\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[target_column] = label_encoder.fit_transform(data[target_column])\n",
    "    \n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y, label_encoder\n",
    "\n",
    "# Stratified sampling by class percentage\n",
    "def stratified_sampling(X, y, sample_percentage):\n",
    "    data = pd.concat([X, y], axis=1)\n",
    "    sampled_data = data.groupby(y.name, group_keys=False).apply(lambda x: x.sample(frac=sample_percentage, random_state=42))\n",
    "    X_sampled = sampled_data.iloc[:, :-1]\n",
    "    y_sampled = sampled_data.iloc[:, -1]\n",
    "    return X_sampled, y_sampled\n",
    "\n",
    "# Build MLP model\n",
    "def build_mlp(input_dim, n_classes):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(n_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='sparse_categorical_crossentropy',  # For integer labels\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, X_test, y_test, n_classes, is_keras=False):\n",
    "    if is_keras:\n",
    "        y_pred_probs = model.predict(X_test)\n",
    "        y_pred = y_pred_probs.argmax(axis=1)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_probs = model.predict_proba(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    auc = roc_auc_score(y_test, y_pred_probs, multi_class='ovr', average='weighted') if n_classes > 2 else None\n",
    "    return {'Accuracy': acc, 'F1 Score': f1, 'AUC': auc}\n",
    "\n",
    "# Save results to log\n",
    "def log_results(log_file, model_name, train_dataset, test_dataset, metrics, train_time):\n",
    "    log_entry = {\n",
    "        \"model\": model_name,\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"test_dataset\": test_dataset,\n",
    "        \"metrics\": metrics,\n",
    "        \"train_time\": train_time\n",
    "    }\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(json.dumps(log_entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09d2cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments with 10% of the data per class...\n",
      "Training Random Forest on dataset1_10% and testing on dataset2_10% (test split)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khang\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\khang\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost on dataset1_10% and testing on dataset2_10% (test split)\n",
      "Training LightGBM on dataset1_10% and testing on dataset2_10% (test split)\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 99428, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -0.254499\n",
      "[LightGBM] [Info] Start training from score -2.727939\n",
      "[LightGBM] [Info] Start training from score -4.826334\n",
      "[LightGBM] [Info] Start training from score -2.226856\n",
      "[LightGBM] [Info] Start training from score -4.884453\n",
      "[LightGBM] [Info] Start training from score -5.708096\n",
      "[LightGBM] [Info] Start training from score -8.462667\n",
      "[LightGBM] [Info] Start training from score -3.488564\n",
      "[LightGBM] [Info] Start training from score -6.308692\n",
      "Training MLP on dataset1_10% and testing on dataset2_10% (test split)\n",
      "6215/6215 [==============================] - 3s 528us/step\n",
      "Running experiments with 20% of the data per class...\n",
      "Training Random Forest on dataset1_20% and testing on dataset2_20% (test split)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khang\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\khang\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost on dataset1_20% and testing on dataset2_20% (test split)\n",
      "Training LightGBM on dataset1_20% and testing on dataset2_20% (test split)\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15099\n",
      "[LightGBM] [Info] Number of data points in the train set: 198855, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -0.254488\n",
      "[LightGBM] [Info] Start training from score -2.728011\n",
      "[LightGBM] [Info] Start training from score -4.826329\n",
      "[LightGBM] [Info] Start training from score -2.226851\n",
      "[LightGBM] [Info] Start training from score -4.885113\n",
      "[LightGBM] [Info] Start training from score -5.706577\n",
      "[LightGBM] [Info] Start training from score -8.462662\n",
      "[LightGBM] [Info] Start training from score -3.488559\n",
      "[LightGBM] [Info] Start training from score -6.311453\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training MLP on dataset1_20% and testing on dataset2_20% (test split)\n",
      "6215/6215 [==============================] - 4s 598us/step\n",
      "Running experiments with 40% of the data per class...\n",
      "Training Random Forest on dataset1_40% and testing on dataset2_40% (test split)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khang\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\khang\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost on dataset1_40% and testing on dataset2_40% (test split)\n",
      "Training LightGBM on dataset1_40% and testing on dataset2_40% (test split)\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15100\n",
      "[LightGBM] [Info] Number of data points in the train set: 397711, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -0.254490\n",
      "[LightGBM] [Info] Start training from score -2.728014\n",
      "[LightGBM] [Info] Start training from score -4.826332\n",
      "[LightGBM] [Info] Start training from score -2.226854\n",
      "[LightGBM] [Info] Start training from score -4.885115\n",
      "[LightGBM] [Info] Start training from score -5.706580\n",
      "[LightGBM] [Info] Start training from score -8.462664\n",
      "[LightGBM] [Info] Start training from score -3.488479\n",
      "[LightGBM] [Info] Start training from score -6.311456\n",
      "Training MLP on dataset1_40% and testing on dataset2_40% (test split)\n",
      "6215/6215 [==============================] - 3s 509us/step\n",
      "Running experiments with 80% of the data per class...\n",
      "Training Random Forest on dataset1_80% and testing on dataset2_80% (test split)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khang\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\khang\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost on dataset1_80% and testing on dataset2_80% (test split)\n",
      "Training LightGBM on dataset1_80% and testing on dataset2_80% (test split)\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15099\n",
      "[LightGBM] [Info] Number of data points in the train set: 795424, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -0.254494\n",
      "[LightGBM] [Info] Start training from score -2.727997\n",
      "[LightGBM] [Info] Start training from score -4.826178\n",
      "[LightGBM] [Info] Start training from score -2.226856\n",
      "[LightGBM] [Info] Start training from score -4.885118\n",
      "[LightGBM] [Info] Start training from score -5.706961\n",
      "[LightGBM] [Info] Start training from score -8.456732\n",
      "[LightGBM] [Info] Start training from score -3.488481\n",
      "[LightGBM] [Info] Start training from score -6.310766\n",
      "Training MLP on dataset1_80% and testing on dataset2_80% (test split)\n",
      "6215/6215 [==============================] - 3s 506us/step\n",
      "Experiment results with multiclass classification logged in C:\\Final Project in Uni\\CTGAN\\expe/tabgandataset/2017experiment_log_ctabgan2017.json\n"
     ]
    }
   ],
   "source": [
    "target_column = 'Target'  # Replace with your target column\n",
    "log_file = 'C:\\Final Project in Uni\\CTGAN\\expe/tabgandataset/2017experiment_log_ctabgan2017.json'\n",
    "sample_percentages = [0.1, 0.2, 0.4, 0.8]  # Percentages for stratified sampling\n",
    "    \n",
    "    # Load datasets\n",
    "data1, data2 = load_datasets()\n",
    "X1, y1, label_encoder1 = preprocess_data(data1, target_column)\n",
    "X2, y2, label_encoder2 = preprocess_data(data2, target_column)\n",
    "n_classes = len(label_encoder1.classes_)  # Number of classes\n",
    "    \n",
    "    # Models to train\n",
    "models = {\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "        \"MLP\": None  # Special handling for MLP\n",
    "    }\n",
    "\n",
    "    # Loop through sample percentages\n",
    "for sample_percentage in sample_percentages:\n",
    "    print(f\"Running experiments with {int(sample_percentage * 100)}% of the data per class...\")\n",
    "    \n",
    "    # Stratified sampling\n",
    "    X1_sampled, y1_sampled = stratified_sampling(X1, y1, sample_percentage)\n",
    "    #X2_sampled, y2_sampled = stratified_sampling(X2, y2, sample_percentage)\n",
    "\n",
    "    # Split dataset 2 into train/test for evaluation\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
    "        X2, y2, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale data for MLP\n",
    "    scaler1 = StandardScaler()\n",
    "    scaler2 = StandardScaler()\n",
    "    X1_scaled = scaler1.fit_transform(X1_sampled)\n",
    "    X2_train_scaled = scaler2.fit_transform(X2_train)\n",
    "    X2_test_scaled = scaler2.transform(X2_test)\n",
    "\n",
    "    # **Train on dataset 1 and test on test split of dataset 2**\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on dataset1_{int(sample_percentage * 100)}% and testing on dataset2_{int(sample_percentage * 100)}% (test split)\")\n",
    "        start_time = time.time()  # Start timer\n",
    "        if model_name == \"MLP\":\n",
    "            mlp = build_mlp(X1_scaled.shape[1], n_classes)\n",
    "            mlp.fit(X1_scaled, y1_sampled, epochs=10, batch_size=32, verbose=0)\n",
    "            train_time = time.time() - start_time  # End timer\n",
    "            metrics = evaluate_model(mlp, X2_test_scaled, y2_test, n_classes, is_keras=True)\n",
    "        else:\n",
    "            model.fit(X1_sampled, y1_sampled)\n",
    "            train_time = time.time() - start_time  # End timer\n",
    "            metrics = evaluate_model(model, X2_test_scaled, y2_test, n_classes)\n",
    "        log_results(log_file, model_name, f\"ctabgam17_{int(sample_percentage * 100)}%\", f\"realdata17_{int(sample_percentage * 100)}% (test split)\", metrics, train_time)\n",
    "\n",
    "    # **Train and test entirely on dataset 2 (train/test split)**\n",
    "    # for model_name, model in models.items():\n",
    "    #     print(f\"Training and testing {model_name} entirely on dataset2_{int(sample_percentage * 100)}% (train/test split)\")\n",
    "    #     start_time = time.time()  # Start timer\n",
    "    #     if model_name == \"MLP\":\n",
    "    #         mlp = build_mlp(X2_train_scaled.shape[1], n_classes)\n",
    "    #         mlp.fit(X2_train_scaled, y2_train, epochs=10, batch_size=32, verbose=0)\n",
    "    #         train_time = time.time() - start_time  # End timer\n",
    "    #         metrics = evaluate_model(mlp, X2_test, y2_test, n_classes, is_keras=True)\n",
    "    #     else:\n",
    "    #         model.fit(X2_train, y2_train)\n",
    "    #         train_time = time.time() - start_time  # End timer\n",
    "    #         metrics = evaluate_model(model, X2_test, y2_test, n_classes)\n",
    "    #     log_results(log_file, model_name, f\"ctabgan20_{int(sample_percentage * 100)}% (train)\", f\"realdata20_{int(sample_percentage * 100)}% (test)\", metrics, train_time)\n",
    "\n",
    "print(f\"Experiment results with multiclass classification logged in {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
